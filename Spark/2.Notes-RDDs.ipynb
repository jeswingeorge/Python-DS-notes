{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\Spark\\\\spark-3.0.0-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5.1</th>\n",
       "      <th>3.5</th>\n",
       "      <th>1.4</th>\n",
       "      <th>0.2</th>\n",
       "      <th>setosa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   5.1  3.5  1.4  0.2  setosa\n",
       "0  4.9  3.0  1.4  0.2  setosa\n",
       "1  4.7  3.2  1.3  0.2  setosa\n",
       "2  4.6  3.1  1.5  0.2  setosa\n",
       "3  5.0  3.6  1.4  0.2  setosa\n",
       "4  5.4  3.9  1.7  0.4  setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('iris/iris_site.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['5.1', '3.5', '1.4', '0.2', 'setosa'],\n",
       " ['4.9', '3.0', '1.4', '0.2', 'setosa'],\n",
       " ['4.7', '3.2', '1.3', '0.2', 'setosa'],\n",
       " ['4.6', '3.1', '1.5', '0.2', 'setosa'],\n",
       " ['5.0', '3.6', '1.4', '0.2', 'setosa']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1 = sc.textFile('iris/iris_site.csv').map(lambda line:line.split(','))\n",
    "iris1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting a particular column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.1', '4.9', '4.7', '4.6', '5.0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod = iris1.map(lambda col:col[0])\n",
    "iris1_mod.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using for-loop with RDD object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1 <class 'float'>\n",
      "4.9 <class 'float'>\n",
      "4.7 <class 'float'>\n",
      "4.6 <class 'float'>\n",
      "5.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "for var1 in iris1.map(lambda col: float(col[0])).take(5):\n",
    "    print(var1, type(var1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typecasting all column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5.1, 3.5, 1.4, 0.2, 'setosa'],\n",
       " [4.9, 3.0, 1.4, 0.2, 'setosa'],\n",
       " [4.7, 3.2, 1.3, 0.2, 'setosa'],\n",
       " [4.6, 3.1, 1.5, 0.2, 'setosa'],\n",
       " [5.0, 3.6, 1.4, 0.2, 'setosa']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod = iris1.map(lambda var: [float(var[0]), float(var[1]), float(var[2]), float(var[3]), var[4]])\n",
    "iris1_mod.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating key-value pairs in RDD\n",
    "\n",
    "Cases where data must be in key-value pair: \n",
    "- key - name of column\n",
    "- value- corresponding values for a particular column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Sepal Length', 5.1),\n",
       "  ('Sepal Width', 3.5),\n",
       "  ('Petal Length', 1.4),\n",
       "  ('Petal Width', 0.2),\n",
       "  ('Species', 'setosa')),\n",
       " (('Sepal Length', 4.9),\n",
       "  ('Sepal Width', 3.0),\n",
       "  ('Petal Length', 1.4),\n",
       "  ('Petal Width', 0.2),\n",
       "  ('Species', 'setosa')),\n",
       " (('Sepal Length', 4.7),\n",
       "  ('Sepal Width', 3.2),\n",
       "  ('Petal Length', 1.3),\n",
       "  ('Petal Width', 0.2),\n",
       "  ('Species', 'setosa')),\n",
       " (('Sepal Length', 4.6),\n",
       "  ('Sepal Width', 3.1),\n",
       "  ('Petal Length', 1.5),\n",
       "  ('Petal Width', 0.2),\n",
       "  ('Species', 'setosa')),\n",
       " (('Sepal Length', 5.0),\n",
       "  ('Sepal Width', 3.6),\n",
       "  ('Petal Length', 1.4),\n",
       "  ('Petal Width', 0.2),\n",
       "  ('Species', 'setosa'))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod = iris1.map(lambda var: (('Sepal Length', float(var[0])),\n",
    "                       ('Sepal Width', float(var[1])),\n",
    "                       ('Petal Length', float(var[2])),\n",
    "                       ('Petal Width', float(var[3])),\n",
    "                       ('Species', var[4])))\n",
    "\n",
    "iris1_mod.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observe:__ The key-value pair is present inside a double collection.\n",
    "\n",
    "To remove the extra level of collection `flatMap` is used instead of `Map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sepal Length', 5.1),\n",
       " ('Sepal Width', 3.5),\n",
       " ('Petal Length', 1.4),\n",
       " ('Petal Width', 0.2),\n",
       " ('Species', 'setosa')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod = iris1.flatMap(lambda var: (('Sepal Length', float(var[0])),\n",
    "                       ('Sepal Width', float(var[1])),\n",
    "                       ('Petal Length', float(var[2])),\n",
    "                       ('Petal Width', float(var[3])),\n",
    "                       ('Species', var[4])))\n",
    "\n",
    "iris1_mod.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5.1, 3.5, 1.4, 0.2, 'setosa'],\n",
       " [4.9, 3.0, 1.4, 0.2, 'setosa'],\n",
       " [4.7, 3.2, 1.3, 0.2, 'setosa'],\n",
       " [4.6, 3.1, 1.5, 0.2, 'setosa'],\n",
       " [5.0, 3.6, 1.4, 0.2, 'setosa']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod = iris1.map(lambda var: [float(var[0]), float(var[1]), float(var[2]), float(var[3]), var[4]])\n",
    "iris1_mod.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4.3, 3.0, 1.1, 0.1, 'setosa'],\n",
       " [4.4, 2.9, 1.4, 0.2, 'setosa'],\n",
       " [4.4, 3.0, 1.3, 0.2, 'setosa'],\n",
       " [4.4, 3.2, 1.3, 0.2, 'setosa'],\n",
       " [4.5, 2.3, 1.3, 0.3, 'setosa']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod.sortBy(lambda x:x[0]).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting based on key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['setosa', ('5.1', '3.5', '1.4', '0.2')],\n",
       " ['setosa', ('4.9', '3.0', '1.4', '0.2')],\n",
       " ['setosa', ('4.7', '3.2', '1.3', '0.2')],\n",
       " ['setosa', ('4.6', '3.1', '1.5', '0.2')],\n",
       " ['setosa', ('5.0', '3.6', '1.4', '0.2')]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod = iris1.map(lambda var: [var[4], (var[0], var[1], var[2], var[3])])\n",
    "\n",
    "iris1_mod.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['virginica', ('7.9', '3.8', '6.4', '2.0')],\n",
       " ['virginica', ('7.7', '3.8', '6.7', '2.2')],\n",
       " ['virginica', ('7.7', '3.0', '6.1', '2.3')],\n",
       " ['virginica', ('7.7', '2.8', '6.7', '2.0')],\n",
       " ['virginica', ('7.7', '2.6', '6.9', '2.3')]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod.sortBy(ascending = False, keyfunc = lambda k:k).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Species', 'setosa'],\n",
       " ['Species', 'setosa'],\n",
       " ['Species', 'setosa'],\n",
       " ['Species', 'setosa'],\n",
       " ['Species', 'setosa']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl = iris1.map(lambda var1:['Sepal Length', float(var1[0])])\n",
    "sw = iris1.map(lambda var1:['Sepal Width', float(var1[1])])\n",
    "pl = iris1.map(lambda var1:['Petal Length', float(var1[2])])\n",
    "pw = iris1.map(lambda var1:['Petal Width', float(var1[3])])\n",
    "sp = iris1.map(lambda var1:['Species', var1[4]])\n",
    "sp.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Sepal Length', 5.1],\n",
       " ['Sepal Length', 4.9],\n",
       " ['Sepal Length', 4.7],\n",
       " ['Sepal Length', 4.6],\n",
       " ['Sepal Length', 5.0]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_data = sp.union(sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\spark_union.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection() / intersect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', 'e']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize(['a', 'b', 'c', 'd', 'e'])\n",
    "y = sc.parallelize(['d', 'e', 'f', 'g', 'h'])\n",
    "x.intersection(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc1 = sc.parallelize([('emp1', [3,1,2,5]), ('emp2', [2,4,1,5,7])])\n",
    "loc2 = sc.parallelize([('emp2', [15,7,3,1]), ('emp1', [6,9,1,3,0])])\n",
    "join1 = loc1.join(loc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp2', ([2, 4, 1, 5, 7], [15, 7, 3, 1])),\n",
       " ('emp1', ([3, 1, 2, 5], [6, 9, 1, 3, 0]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through values - `foreach`\n",
    "\n",
    "An RDD will contain multiple elements. If a certain operation needs to be performed on each element of RDD, we can make use of iterative loops like `foreach`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.1, 4.5, 2.4, 1.2, 5.9]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod = iris1.flatMap(lambda var: (('Sepal Length', float(var[0])),\n",
    "                       ('Sepal Width', float(var[1])),\n",
    "                       ('Petal Length', float(var[2])),\n",
    "                       ('Petal Width', float(var[3]))))\n",
    "\n",
    "def fun(x): \n",
    "    print(x)\n",
    "    \n",
    "# iris1_mod.foreach(fun).take(5)   # This is giving error AttributeError: 'NoneType' object has no attribute 'take'\n",
    "\n",
    "iris1_mod.map(lambda x: x[1]+1).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sepal Length', 5.1),\n",
       " ('Sepal Width', 3.5),\n",
       " ('Petal Length', 1.4),\n",
       " ('Petal Width', 0.2)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering data based on a condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['7.1', '3.0', '5.9', '2.1', 'virginica'],\n",
       " ['7.6', '3.0', '6.6', '2.1', 'virginica'],\n",
       " ['7.3', '2.9', '6.3', '1.8', 'virginica'],\n",
       " ['7.2', '3.6', '6.1', '2.5', 'virginica'],\n",
       " ['7.7', '3.8', '6.7', '2.2', 'virginica']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all records whose Sepal_Length (1st column value) is less than 7\n",
    "f1 = iris1.filter(lambda x: float(x[0])>7)\n",
    "f1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7.0', '6.9', '6.6', '6.7', '6.6']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to print onlt first column values and values >6.5\n",
    "f2 = iris1.filter(lambda x: float(x[0])>6.5).map(lambda x: x[0])\n",
    "f2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Combining multiple RDD data with same key\n",
    "\n",
    "When there are key-value pairs in two seperate RDDs and we are required to collate all data with the same key together from both RDDs.  \n",
    "\n",
    "To achieve the same we can amke use of `cogroup()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1bfd3129608>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1bfd319fcc8>)),\n",
       " ('c',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1bfd30ae608>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1bfd30ff0c8>)),\n",
       " ('a',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1bfd3134ac8>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1bfd3134908>))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = sc.parallelize([('a', 1), ('b', 2)])\n",
    "p2 = sc.parallelize([('a', 99), ('b', 80), ('c', 100)])\n",
    "p1.cogroup(p2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that for each key there is a collection of value as a result of cogroup function.\n",
    "\n",
    "Also  \n",
    "- Result of cogroup function can be realized by combining for loop with tuple, map and list function.\n",
    "- for each key we get an iterable object.\n",
    "- Using for loop we iterate through each key,value pair of the cogroup result and we place each value in a list, and the collection of the list is in turn placed inside a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', ([2], [80])), ('c', ([], [100])), ('a', ([1], [99]))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x,tuple(map(list,y)))  for x,y in p1.cogroup(p2).collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case when there are key-value pairs in multiple RDDs and we are required to collate all data with the same key together from all RDDs. Done using `groupWith` function.\n",
    "\n",
    "`groupWith`works exactly like `cogroup` except that it deals with more RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How to pass list of RDDs to groupWith in Pyspark](https://stackoverflow.com/questions/37621853/how-to-pass-list-of-rdds-to-groupwith-in-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', ([6], [], [], [999])),\n",
       " ('c', ([], [5], [], [])),\n",
       " ('a', ([5], [], [100], [])),\n",
       " ('d', ([], [6], [], [1000]))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = sc.parallelize([('a',5), ('b', 6)])\n",
    "d2 = sc.parallelize([('c',5), ('d', 6)])\n",
    "d3 = sc.parallelize([('a', 100)])\n",
    "d4 = sc.parallelize([('d', 1000), ('b', 999)])\n",
    "\n",
    "m = [d1,d2,d3,d4]\n",
    "\n",
    "gw = m[0].groupWith(*m[1:])  # Since groupWith accepts varargs all you have to do is to unpack arguments\n",
    "\n",
    "[(x,tuple(map(list,y)))  for x,y in gw.collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting occurence\n",
    "\n",
    "### 1. Count occurence of each key element\n",
    "\n",
    "To count the key element `countByKey` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('a', 2), ('b', 1), ('c', 2)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = sc.parallelize([('a', 5), ('b', 12), ('a', 23), ('c', 87), ('c', 65)])\n",
    "p1.countByKey().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('setosa', 50), ('versicolor', 50), ('virginica', 50)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod = iris1.map(lambda x:(x[4],1))\n",
    "iris1_mod.countByKey().items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Count occurence of each value\n",
    "\n",
    "Count of distinct occurence of each value in a particular column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('setosa', 50), ('versicolor', 50), ('virginica', 50)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1.map(lambda x:x[4]).countByValue().items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting total number of values in a particular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['5.1', '3.5', '1.4', '0.2', 'setosa'],\n",
       " ['4.9', '3.0', '1.4', '0.2', 'setosa'],\n",
       " ['4.7', '3.2', '1.3', '0.2', 'setosa'],\n",
       " ['4.6', '3.1', '1.5', '0.2', 'setosa'],\n",
       " ['5.0', '3.6', '1.4', '0.2', 'setosa']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each row of the iris dataset is imported as a list so there are 150 lists within the master list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct\n",
    "\n",
    "Distinct occurence of values in a particular column can be calculated using the distinct function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setosa', 'versicolor', 'virginica']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod = iris1.map(lambda x:x[4])\n",
    "distinct1 = iris1_mod.distinct()\n",
    "distinct1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating sequence of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range1 = sc.range(start = 1, end = 10, step = 2)\n",
    "type(range1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply function for each key - Using `mapValues()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = sc.parallelize([('SL', [2,1,3,5,1]), ('SW', [5,6,1,3,0])])\n",
    "\n",
    "def add_vals(para):\n",
    "    return(sum(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SL', 12), ('SW', 15)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.mapValues(add_vals).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Aggregation\n",
    "\n",
    "### 1. Measure data in RDD cab be aggregated using `fold` function\n",
    "\n",
    "__Reference:__ [operator module in python](https://docs.python.org/3/library/operator.html)\n",
    "\n",
    "[fold operations in spark](https://www.edureka.co/community/11994/can-anyone-explain-fold-operation-in-spark)\n",
    "\n",
    "\n",
    "`operator.add(x, y)` == `x+y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add all values of Sepal Width column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458.60000000000014"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1.map(lambda x:float(x[1])).fold(0, add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. To aggregate the value for each key\n",
    "Used when data is in key-value format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sepal Length', 5.1),\n",
       " ('Sepal Width', 3.5),\n",
       " ('Petal Length', 1.4),\n",
       " ('Petal Width', 0.2),\n",
       " ('Sepal Length', 4.9)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod = iris1.flatMap(lambda var: (('Sepal Length', float(var[0])),\n",
    "                       ('Sepal Width', float(var[1])),\n",
    "                       ('Petal Length', float(var[2])),\n",
    "                       ('Petal Width', float(var[3]))))\n",
    "\n",
    "iris1_mod.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sepal Length', 876.5000000000002),\n",
       " ('Sepal Width', 458.60000000000014),\n",
       " ('Petal Length', 563.7000000000004),\n",
       " ('Petal Width', 179.90000000000012)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod.foldByKey(0, add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce\n",
    "\n",
    "- Function used to reduce the elements of an RDD (usually used for aggregation).\n",
    "- Argument is function to be executed on RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "876.5000000000002"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod = iris1.map(lambda x: float(x[0]))\n",
    "iris1_mod.reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey()\n",
    "- Used where it is required to aggregate each group of data\n",
    "- function passed by the function is applied for all values in particular key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sepal Length', 876.5000000000002),\n",
       " ('Sepal Width', 458.60000000000014),\n",
       " ('Petal Length', 563.7000000000004),\n",
       " ('Petal Width', 179.90000000000012)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod = iris1.flatMap(lambda var: (('Sepal Length', float(var[0])),\n",
    "                       ('Sepal Width', float(var[1])),\n",
    "                       ('Petal Length', float(var[2])),\n",
    "                       ('Petal Width', float(var[3]))))\n",
    "\n",
    "iris1_mod.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey()\n",
    "\n",
    "We can group elements with the same key using `groupByKey()` function. On applying the function we group data for each column.\n",
    "\n",
    "The grouped result is an iterable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sepal Length', 5.1),\n",
       " ('Sepal Width', 3.5),\n",
       " ('Petal Length', 1.4),\n",
       " ('Petal Width', 0.2),\n",
       " ('Sepal Length', 4.9)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod = iris1.flatMap(lambda var: (('Sepal Length', float(var[0])),\n",
    "                       ('Sepal Width', float(var[1])),\n",
    "                       ('Petal Length', float(var[2])),\n",
    "                       ('Petal Width', float(var[3]))))\n",
    "\n",
    "iris1_mod.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sepal Length', <pyspark.resultiterable.ResultIterable at 0x1bfd319a908>),\n",
       " ('Sepal Width', <pyspark.resultiterable.ResultIterable at 0x1bfd319aac8>),\n",
       " ('Petal Length', <pyspark.resultiterable.ResultIterable at 0x1bfd319a708>),\n",
       " ('Petal Width', <pyspark.resultiterable.ResultIterable at 0x1bfd319ac88>)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `mapValues()` to convert iterable objects to list or any other form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sepal Length',\n",
       "  [5.1,\n",
       "   4.9,\n",
       "   4.7,\n",
       "   4.6,\n",
       "   5.0,\n",
       "   5.4,\n",
       "   4.6,\n",
       "   5.0,\n",
       "   4.4,\n",
       "   4.9,\n",
       "   5.4,\n",
       "   4.8,\n",
       "   4.8,\n",
       "   4.3,\n",
       "   5.8,\n",
       "   5.7,\n",
       "   5.4,\n",
       "   5.1,\n",
       "   5.7,\n",
       "   5.1,\n",
       "   5.4,\n",
       "   5.1,\n",
       "   4.6,\n",
       "   5.1,\n",
       "   4.8,\n",
       "   5.0,\n",
       "   5.0,\n",
       "   5.2,\n",
       "   5.2,\n",
       "   4.7,\n",
       "   4.8,\n",
       "   5.4,\n",
       "   5.2,\n",
       "   5.5,\n",
       "   4.9,\n",
       "   5.0,\n",
       "   5.5,\n",
       "   4.9,\n",
       "   4.4,\n",
       "   5.1,\n",
       "   5.0,\n",
       "   4.5,\n",
       "   4.4,\n",
       "   5.0,\n",
       "   5.1,\n",
       "   4.8,\n",
       "   5.1,\n",
       "   4.6,\n",
       "   5.3,\n",
       "   5.0,\n",
       "   7.0,\n",
       "   6.4,\n",
       "   6.9,\n",
       "   5.5,\n",
       "   6.5,\n",
       "   5.7,\n",
       "   6.3,\n",
       "   4.9,\n",
       "   6.6,\n",
       "   5.2,\n",
       "   5.0,\n",
       "   5.9,\n",
       "   6.0,\n",
       "   6.1,\n",
       "   5.6,\n",
       "   6.7,\n",
       "   5.6,\n",
       "   5.8,\n",
       "   6.2,\n",
       "   5.6,\n",
       "   5.9,\n",
       "   6.1,\n",
       "   6.3,\n",
       "   6.1,\n",
       "   6.4,\n",
       "   6.6,\n",
       "   6.8,\n",
       "   6.7,\n",
       "   6.0,\n",
       "   5.7,\n",
       "   5.5,\n",
       "   5.5,\n",
       "   5.8,\n",
       "   6.0,\n",
       "   5.4,\n",
       "   6.0,\n",
       "   6.7,\n",
       "   6.3,\n",
       "   5.6,\n",
       "   5.5,\n",
       "   5.5,\n",
       "   6.1,\n",
       "   5.8,\n",
       "   5.0,\n",
       "   5.6,\n",
       "   5.7,\n",
       "   5.7,\n",
       "   6.2,\n",
       "   5.1,\n",
       "   5.7,\n",
       "   6.3,\n",
       "   5.8,\n",
       "   7.1,\n",
       "   6.3,\n",
       "   6.5,\n",
       "   7.6,\n",
       "   4.9,\n",
       "   7.3,\n",
       "   6.7,\n",
       "   7.2,\n",
       "   6.5,\n",
       "   6.4,\n",
       "   6.8,\n",
       "   5.7,\n",
       "   5.8,\n",
       "   6.4,\n",
       "   6.5,\n",
       "   7.7,\n",
       "   7.7,\n",
       "   6.0,\n",
       "   6.9,\n",
       "   5.6,\n",
       "   7.7,\n",
       "   6.3,\n",
       "   6.7,\n",
       "   7.2,\n",
       "   6.2,\n",
       "   6.1,\n",
       "   6.4,\n",
       "   7.2,\n",
       "   7.4,\n",
       "   7.9,\n",
       "   6.4,\n",
       "   6.3,\n",
       "   6.1,\n",
       "   7.7,\n",
       "   6.3,\n",
       "   6.4,\n",
       "   6.0,\n",
       "   6.9,\n",
       "   6.7,\n",
       "   6.9,\n",
       "   5.8,\n",
       "   6.8,\n",
       "   6.7,\n",
       "   6.7,\n",
       "   6.3,\n",
       "   6.5,\n",
       "   6.2,\n",
       "   5.9])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod.groupByKey().mapValues(list).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating grouped RDD\n",
    "\n",
    "Aggregation can be performed on grouped data by combining the aggregate function with `mapValues()` and `groupByKey()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sepal Length', 876.5000000000002),\n",
       " ('Sepal Width', 458.60000000000014),\n",
       " ('Petal Length', 563.7000000000004),\n",
       " ('Petal Width', 179.90000000000012)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1_mod.groupByKey().mapValues(sum).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Descriptive statistics\n",
    "\n",
    "### 1. max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1.map(lambda x: float(x[1])).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1.map(lambda x: float(x[1])).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0573333333333315"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1.map(lambda x: float(x[1])).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring spread of data\n",
    "\n",
    "### 4. std dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4344109677354942"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1.map(lambda x: float(x[1])).stdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18871288888888857"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1.map(lambda x: float(x[1])).variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Summary of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 150, mean: 3.0573333333333315, stdev: 0.4344109677354942, max: 4.4, min: 2.0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris1.map(lambda x: float(x[1])).stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. subtractByKey()\n",
    "\n",
    "Return each (key, value) pair in self that has no pair with matching key in other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 5)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([('a',1), ('b',5),('a',5)])\n",
    "y = sc.parallelize([('a',3), ('c',None)])\n",
    "x.subtractByKey(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. subtract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 5)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([('a',1), ('b',5),('a',3)])\n",
    "y = sc.parallelize([('a',3), ('c',None), ('b', 2)])\n",
    "x.subtract(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
