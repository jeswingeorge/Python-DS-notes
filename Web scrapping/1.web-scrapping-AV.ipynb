{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "1. [Analytics Vidhya: Introduction to Web Scraping in Python](https://www.analyticsvidhya.com/blog/2019/10/web-scraping-hands-on-introduction-python/)\n",
    "2. [Data school web-scrapping python](https://www.dataschool.io/python-web-scraping-of-president-trumps-lies/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the web page into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagret url to scrap\n",
    "url = \"https://www.goibibo.com/hotels/find-hotels-in-Pune/1554245012668028405/1554245012668028405/%7B%22ci%22:%2220191025%22,%22co%22:%2220191029%22,%22r%22:%221-2-0%22%7D/?{}&sec=dom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above fetches our web page from the URL, and stores the result in a \"response\" object called `r`. That response object has a `text` attribute, which contains the same HTML code we get when viewing the page source from chrome web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!doctype html>\n",
      "<html lang=\"en-us\">\n",
      "<head>\n",
      "<script>\n",
      "          var starttime = new Date();\n",
      "        </script>\n",
      "<title data-react-helmet=\"true\">Results</title>\n",
      "<meta data-react-helmet=\"true\" name=\"description\" property=\"og:description\" content=\"Goibibo provides you online hotel bookings all over the world. Book cheap, budget and luxury hotels at best price from leading hotel booking site. Free cancellation on many hotels\"/><meta data-react-helmet=\"true\" name=\"keywords\" content=\"Goibibo, online hote\n"
     ]
    }
   ],
   "source": [
    "# print the first 500 characters of the HTML\n",
    "print(r.text[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the HTML using Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above parses the HTML (stored in r.text) into a special object called soup that the Beautiful Soup library understands. In other words, __Beautiful Soup is reading the HTML and making sense of its structure__.\n",
    "\n",
    "(Note that __lxml__ is the parser included with the Python standard library, though other parsers can be used by Beautiful Soup. See [differences between parsers](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#differences-between-parsers) to learn more.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting all of the records\n",
    "\n",
    "Taking advantage of the patterns we noticed in the article formatting to build our dataset. __Each record will be tagged in a consistent way in the HTML. This is the pattern that allows us to build our dataset__.\n",
    "\n",
    "The Beautiful Soup methods required for this task are:\n",
    "\n",
    "1. find()\n",
    "2. find_all()\n",
    "\n",
    "There is an excellent tutorial on these methods [(Searching the tree)](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree) in the Beautiful Soup documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find_all('p', attrs = {'class':'ico20 fb'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code searches the soup object for all `<p>` tags with the attribute __class=\"short-desc\"__. It returns a special Beautiful Soup object (called a \"ResultSet\") containing the search results.\n",
    "\n",
    "__results__ acts like a Python list, so we can check its length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
